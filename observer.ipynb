{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import datetime\n",
    "import time\n",
    "import subprocess\n",
    "from credentials import API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will first define the file path and the batch size\n",
    "### The batch size can be changed according to the requirements and considering how often the data is being updated in the world_bank_preprocessed.csv file.\n",
    "### Setting the batch size to 1000 will call the pipeline function after 1000 entries have been added to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"world_bank_preprocessed.csv\")\n",
    "batch_size = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function that Preprocesses the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    subprocess.check_call(['jupyter', 'nbconvert', '--to', 'notebook', '--execute', 'preprocess.ipynb'])\n",
    "    print(\"Preprocessing stage completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function that updates the prompt completion pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_completion():\n",
    "    subprocess.run(['python', 'generator.py'])\n",
    "    print(\"Prompt Completion stage completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function that retrains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"prompt_completion_pairs.json\"\n",
    "output_file_path = \"prompt_completion_pairs_prepared.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "timestamp = []\n",
    "\n",
    "def retrain(api_key, input_file_path, output_file_path):\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    \n",
    "    cmd_prepare = f\"openai tools fine_tunes.prepare_data -f {input_file_path}\"\n",
    "    subprocess.run(cmd_prepare.split(), check=True)\n",
    "    \n",
    "    cmd_create = f\"openai api fine_tunes.create -t {output_file_path} -m curie\"\n",
    "    result = subprocess.run(cmd_create.split(), check=True, capture_output=True)\n",
    "\n",
    "    model_accuracy = float(result.stdout.decode().split('\\n')[2].split(': ')[1])\n",
    "\n",
    "    accuracy.append(model_accuracy)\n",
    "    timestamp.append(datetime.datetime.now())\n",
    "\n",
    "    print(\"Retraining stage completed!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy():\n",
    "    print(\"Deployment stage completed!\")\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function that is triggered when 1000 entries are added to the dataset (Defining the Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "\n",
    "    print(\"Pipeline function triggered!\")\n",
    "\n",
    "    # THE MODEL PIPELINE\n",
    "\n",
    "    # 1. This Function is called after every 1000 rows are added to the 'word_bank_preprocessed.csv' dataset\n",
    "    # 2. Call the function that preprocesses the new entries exactly how the 'word_bank_preprocessed.csv' has been preprocessed.\n",
    "    # 3. Write the commands to re-train the model\n",
    "    # 4. Call the function that deploys the model in production environment ( Not Necessary as of now )\n",
    "    \n",
    "    preprocess()\n",
    "\n",
    "    time.sleep(10)   # To make sure it is easy to keep a track of the real-time data coming in\n",
    "    \n",
    "    prompt_completion()\n",
    "    retrain(API_KEY, input_file_path, output_file_path)   \n",
    "    deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_csv_file(filepath, batch_size):\n",
    "\n",
    "    initial_rows = pd.read_csv(filepath).shape[0]\n",
    "    \n",
    "    while True:\n",
    "        while pd.read_csv(filepath).shape[0] == initial_rows:\n",
    "            time.sleep(1)\n",
    "            \n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        if df.shape[0] >= initial_rows + batch_size:\n",
    "\n",
    "            pipeline()\n",
    "\n",
    "            trace = go.Scatter(x=[], y=[], mode='lines', name='Model Analysis')\n",
    "            layout = go.Layout(title='Model Analysis', xaxis=dict(title='Timestamp'), yaxis=dict(title='Model Accuracy'))\n",
    "            fig = go.Figure(data=[trace], layout=layout)\n",
    "            pyo.plot(fig, auto_open=False)\n",
    "            pyo.plot(fig, filename=\"Real-Time-Model-Analysis.html\", auto_open=False)\n",
    "            \n",
    "            initial_rows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server():\n",
    "    try:\n",
    "        watch_csv_file(filepath, batch_size)   \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nServer stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline function triggered!\n",
      "Preprocessing stage completed!\n",
      "Prompt Completion stage completed!\n",
      "Deployment stage completed!\n",
      "\n",
      "Server stopped!\n"
     ]
    }
   ],
   "source": [
    "server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
